# Kubernetes

## What makes containerisation possible?

* Linux namespaces
* Linux Control Groups(cgroups)

## Scaling

* Horizontal scaling(scaling out)
* Vertical scaling(scaling up)

## What is K8s?

* Container Orchestration tool
* Developed initially by Google

## Why K8s?

* Trend from monolith to micro services gave rise to increase in usage
  of containerisation technology.
* This resulted in application that now comprise of 100s or 1000s of
  running containers.
* Making managing application through self-made scripts and tools
  difficult.
* Hence, container orchestration tool was born.

## Features

* Self-healing or disaster recovery
* High availability
* Scalability

## Basic Architecture

* One or more master nodes/control plane
* Many worker nodes

## Master Node / Control Plane

### API Server
* Entry point to K8s cluster
* You and other components of control plane interact with `etcd` via API
  Server

### Controller Manager

* Performs cluster-level functions like replicating pods, keeping track
  of worker nodes, handling node failures, and so on.

### Scheduler - decides and ensures pods placement

* Schedules pods
* Assigns a worker node to each deployable component of your application

### `etcd` - Kubernetes backing store(key value store)

* Reliable, distributed, key-value pair data store

## Work Node

### Container runtime

* Docker
* rkt

### Kubelet

* Process which allows nodes to communicate with each other
* Talks to the API server
* Manages containers on its node

### Kube Proxy

* Load-balances network traffic

## Virtual Network

* Creates one unified machine

## Node

* Any machine part of the cluster

## Pod

* Smallest unit in K8s
* Abstraction over container
* Usually contain only one container
* Each pod gets its own IP Address
* Pods can communicate with one another using that IP Address regardless
  of the node they are on.
* Pods are ephemeral
* When a pod dies, it may be possible that the new pod is assigned a new
  IP address and hence the `service` component comes into picture.

## Service

* Permanent IP address
* Life cycle of pod and service not connected
* Internal and External service

## ConfigMap

* For storing non-confidential data only
* Example,
    * `ENV` variables
    * Configuration Parameters

## Secrets

* For storing confidential/secret data,
* Secrets are stored in `base64` encoding
* Meant to be encrypted by 3rd party tools
* Example,
    * Username
    * Password
    * CLIENT_ID
    * CLIENT_SECRET

## Deployment

* Stateless
* Blueprint
* Layer of abstraction over pods
* In practice, you mostly work with `deployment` and not with `pods`
  directly.
* Shouldn't be used for deploying databases as DB is stateful

## Stateful Set

* Like deployment but used for stateful apps like databases
* More difficult than `deployment`. Due to this, it is often a common
  practice to host DB outside K8s cluster.

## Configuration file

* `apiVersion`
* `kind` (deployment/service)
* `metadata`
* `spec` (specification)
    * `replicas`
    * `selector`
    * `template`
* `status`

> `spec` will be specific to the `kind` of component

### Status

* Automatically generated by K8s
* Checks whether the desired state = actual state
* If not, then K8s will try to fix it.
* Basis of the self-healing feature of K8s

### Where does K8s get status data?

* From `etcd`
* `etcd` holds the current status of all K8s component

## Minikube

* `minikube start --driver [docker|kvm2|virtualbox]`
* `minikube status`
* `minikube stop`
* `minikube delete`
* `minikube ip`
* `minikube ssh`
* `minkube service list`

## Kubectl

* Client for interacting with API Server
* `kubectl create` (rarely used)
* `kubectl get [nodes|pods|configmap|secret|replicasets]`
* `kubectl logs`
* `kubectl logs --previous`
* `kubectl describe [source] [resource]`
* `-o wide`
* `kubectl edit`
* `kubectl exec -it nginx-depl-5ddc44dd46-8p668 -- /bin/bash`
* `kubectl delete deployments.apps nginx-depl`
* `kubectl delete -f filename.yaml`
* `kubectl expose`
* `kubectl get pod -l release=staging`
* `kubectl get pod -l release!='production'`
* `kubectl get pod -l '!release'`
* `kubectl get pod -l app=kubia,release=staging`
* `kubectl annotate pod kubia anon="foo bar"`
* `kubectl get pod kubia -o json | jq '.metadata.annotations'`

## Port Forwarding

* `kubectl port-forward kubia 8080:8000`

## Namespaces

* A virtual cluster within the K8s cluster
* `kubectl get namespace`
* `kubectl create namespace newnamespace`
* Used to groups logically related components
* Can't access most resources from another namespace
* Can share `service`

```yaml
db_url: mysql-service.database
```

* Some components like volumes live globally in a cluster and cannot be
  isolated into namespaces.

`kubectl api-resources --namespaced=false`

* Example,

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: configmap
  # namespace: default
  namespace: my-namespace
data:
  key: value
```

## Changing active namespace - `kubens` (don't use)

* `kubens` is a 3rd party tool which allows you to change your default
namespace, eliminating the need of typing `-n mynamespace` every time.
* Part of `kubectx` package
* Instead use,

```bash
kubectl config set-context --current --namespace=<NS>
kubectl config view | grep namespace
```

## Ingress

* When an application is exposed to the outside world, the request first
  goes to the ingress and then to the internal service.
* `minikube addons enable ingress`
* `kubectl get pods -n ingress-nginx`
* `kubectl create secret tls example-com-tls --cert=tls.crt --key=tls.key`
* `curl --cacert tls.crt https://example.com`

## Creating a self-signed certificate

```bash
openssl req -x509 \
    -newkey rsa:4096 -sha256 -nodes \
    -keyout tls.key -out tls.crt \
    -subj "/CN=example.com" \
    -days 365
```

## Point your shell to `minikube's` docker-daemon

```bash
eval $(minikube -p minikube docker-env)
```

## Better way to load local image into Minikube

```bash
minikube image load local-image
```

## Liveness Probe

* Hey Pod are you still alive?
* Properties:

```yaml
initialDelaySeconds: 5 # default 0s (inital delay before probing starts)
timeoutSeconds: 5      # default 1s (if the response time exceeds 5, kill)
periodSeconds: 10      # default 10s (time period between each probe)
failureThreshold: 3    # default 3 (no. of failures before killing)
successThreshold: 1    # default 1
```

> Set `initialDelaySeconds` to account for your app's startup time

* 3 types:

### HTTP GET

* 2XX or 3XX response = Alive

```yaml
livenessProbe:
  httpGet:
    path: /
    port: 8000
```

### TCP Socket

* If connection is established = Alive

### Exec

* Executes an arbitrary command
* Exit status code is 0 = Alive

## Readiness Probe

* YAML manifest is exactly same as that of liveness probe.
* Only ready pods are included in the Endpoints resource.
* Readiness probe only governs the pod's ready state and not it's
  termination.
* Since unready pods are not part of the Endpoints resource, traffic is
  not directed to those pods.

```yaml
readinessProbe:
  exec:
    command:
    - ls
    - /var/ready
  initialDelaySeconds: 0 # inital delay before probing starts
  timeoutSeconds: 1      # if the response time exceeds 5, kill
  periodSeconds: 10      # time period between each probe
  failureThreshold: 3    # no. of failures before killing
  successThreshold: 1
```

## Why create deployment instead of creating pods directly?

* If pods are created manually, they are not managed by the control
  plane components but solely by kubelet.
* If a node dies, kubelet running on that node dies as well.
* Hence kubernetes has no way of rescheduling the pod and is lost
  forever.

## `matchExpressions` selector

* More powerful than `matchLabels`.
* Should contain - key, operator, list of values(optional - depends on
  operator).
* Can specify multiple expressions. All must be true in order to match.

### Operators

* `In` - labels must match one of the specified values
* `NotIn` - labels must **not** match any of the specified values
* `Exists` - pod must include label with the specified key(do not
  specify value)
* `DoesNotExist` - pod must **not** include label with the specified
  key(do not specify value)

## Unlabel a pod

* Put a dash at the end of a label to remove it

```bash
kubectl label pods nginx-566b564c99-v7rjs authorized_by-
```

## DaemonSet

* Used to run a pod on every node or a subset of nodes.
* Usually runs system services
* By default runs the pod on each and every node unless explicitly
  specified
* Pods aren't scattered around unlike a ReplicaSet or a
  ReplicationController.
* If a node goes down, the DaemonSet doesn't cause a pod to be created
  elsewhere. But when a new node joins the cluster, DaemonSet deploys a
  new pod instance on it.
* Some node can be made `unschedulable` preventing pods from being
  deployed to them. A DaemonSet will deploy pods even on such nodes,
  because the `unschedulable` attribute is only used by the scheduler.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ssd-monitor
  namespace: default
  labels:
    app: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: ssd-monitor
        image: ssd-monitor:latest
        imagePullPolicy: IfNotPresent
---
```

## Job

* Running pod that performs a single completable task and terminates
  afterwards.
* Pods can run sequentially or in parallel.

## Cron Job

* Like UNIX cron jobs, but managed by Kubernetes
* When the time is right, Cron Job object creates a Job resource which
  deploys a pod instance as specified in the pod template.

## Node Selector

* Schedule pod(s) to a particular node(s)

`kubectl label node mynode gpu=true`

```yaml
kind: Deployment
spec:
  template:
    spec:
      nodeSelector:
        gpu: true
```

## External Traffic Policy

```yaml
kind: Service
spec:
  externalTrafficPolicy: Local # or `Cluster` (default)
  type: NodePort # only `NodePort` and `LoadBalancer` allowed
```

* `Local` preserves client's IP by eliminating an unneeded network hop,
  which in turn prevents SNAT. However, this comes at a cost of possible
  imbalanced traffic distribution. Also the load balancer’s health
  checking capabilities needs to be configured to only send traffic to
  nodes where the corresponding `NodePort` is responsive.

<https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies>

## Headless Service

* A headless service is a `ClusterIP` service with no Cluster IP
* Pods are able to access the pods backing headless service using its
  FQDN

```yaml
kind: Service
spec:
  clusterIP: None
  selector:
    app: headless
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: http
---
```

## Volumes

### Type of volumes:

Non-persistent

* `emptyDir`
* `gitRepo` (deprecated)

Persistent

* `hostPath`
* `nfs` (network file system)
* Google Compute Engine Persistent Disk (GCE PD)
* AWS Elastic Block Store(AWS EBS)
* Azure Disk

Special

* `configMap`
* `secret`
* `downwardAPI`

Refer [example manifests](./volumes) for Persistent Volumes, Persistent
Volume Claims & Storage Classes (Dynamic Provisioning).

### Access Modes

* ReadWriteOnce    (RWO)
* ReadOnlyMany     (ROX)
* ReadWriteMany    (RWX)
* ReadWriteOncePod (RWOP)

First three are w.r.t nodes.

### Persistent Volume Claims

* Not specifying a `storageClassName` defaults to the `default` storage class
* Setting `storageClassName` to "", makes use of an independently
  deployed persistent volume instead of provisioning a new `PV`.
* Namespaced, unlike Persistent Volumes and Storage Classes.
